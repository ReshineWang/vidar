import sys
import os
import subprocess

sys.path.append("./")
sys.path.append(f"./policy")
sys.path.append("./description/utils")
from envs import CONFIGS_PATH
from envs.utils.create_actor import UnStableError

import numpy as np
from pathlib import Path
from collections import deque
import traceback

import yaml
from datetime import datetime
import importlib
import argparse
import pdb
import re

from description.utils.generate_episode_instructions import *

current_file_path = os.path.abspath(__file__)
parent_directory = os.path.dirname(current_file_path)
ansi_escape = re.compile(r'\x1B\[[0-?]*[ -/]*[@-~]')


def clean_print(*args, **kwargs):
    text = " ".join(str(arg) for arg in args)
    text = ansi_escape.sub('', text)
    print(text, **kwargs)


def class_decorator(task_name):
    envs_module = importlib.import_module(f"envs.{task_name}")
    try:
        env_class = getattr(envs_module, task_name)
        env_instance = env_class()
    except:
        raise SystemExit("No Task")
    return env_instance


def eval_function_decorator(policy_name, model_name):
    try:
        policy_model = importlib.import_module(policy_name)
        return getattr(policy_model, model_name)
    except ImportError as e:
        raise e

def get_camera_config(camera_type):
    camera_config_path = os.path.join(parent_directory, "../task_config/_camera_config.yml")

    assert os.path.isfile(camera_config_path), "task config file is missing"

    with open(camera_config_path, "r", encoding="utf-8") as f:
        args = yaml.load(f.read(), Loader=yaml.FullLoader)

    assert camera_type in args, f"camera {camera_type} is not defined"
    return args[camera_type]


def get_embodiment_config(robot_file):
    robot_config_file = os.path.join(robot_file, "config.yml")
    with open(robot_config_file, "r", encoding="utf-8") as f:
        embodiment_args = yaml.load(f.read(), Loader=yaml.FullLoader)
    return embodiment_args


def main(usr_args):
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    task_name = usr_args["task_name"]
    task_config = usr_args["task_config"]
    ckpt_setting = usr_args["ckpt_setting"]
    # checkpoint_num = usr_args['checkpoint_num']
    policy_name = usr_args["policy_name"]
    instruction_type = usr_args["instruction_type"]
    save_dir = Path(usr_args["save_dir"])
    video_save_dir = None
    video_size = None

    get_model = eval_function_decorator(policy_name, "get_model")

    with open(f"./task_config/{task_config}.yml", "r", encoding="utf-8") as f:
        args = yaml.load(f.read(), Loader=yaml.FullLoader)

    args['task_name'] = task_name
    args["task_config"] = task_config
    args["ckpt_setting"] = ckpt_setting
    args["gt_data_dir"] = usr_args.get("gt_data_dir", "")

    embodiment_type = args.get("embodiment")
    embodiment_config_path = os.path.join(CONFIGS_PATH, "_embodiment_config.yml")

    with open(embodiment_config_path, "r", encoding="utf-8") as f:
        _embodiment_types = yaml.load(f.read(), Loader=yaml.FullLoader)

    def get_embodiment_file(embodiment_type):
        robot_file = _embodiment_types[embodiment_type]["file_path"]
        if robot_file is None:
            raise "No embodiment files"
        return robot_file

    with open(CONFIGS_PATH + "_camera_config.yml", "r", encoding="utf-8") as f:
        _camera_config = yaml.load(f.read(), Loader=yaml.FullLoader)

    head_camera_type = args["camera"]["head_camera_type"]
    args["head_camera_h"] = _camera_config[head_camera_type]["h"]
    args["head_camera_w"] = _camera_config[head_camera_type]["w"]

    if len(embodiment_type) == 1:
        args["left_robot_file"] = get_embodiment_file(embodiment_type[0])
        args["right_robot_file"] = get_embodiment_file(embodiment_type[0])
        args["dual_arm_embodied"] = True
    elif len(embodiment_type) == 3:
        args["left_robot_file"] = get_embodiment_file(embodiment_type[0])
        args["right_robot_file"] = get_embodiment_file(embodiment_type[1])
        args["embodiment_dis"] = embodiment_type[2]
        args["dual_arm_embodied"] = False
    else:
        raise "embodiment items should be 1 or 3"

    args["left_embodiment_config"] = get_embodiment_config(args["left_robot_file"])
    args["right_embodiment_config"] = get_embodiment_config(args["right_robot_file"])

    if len(embodiment_type) == 1:
        embodiment_name = str(embodiment_type[0])
    else:
        embodiment_name = str(embodiment_type[0]) + "+" + str(embodiment_type[1])

    save_dir.mkdir(parents=True, exist_ok=True)

    if args["eval_video_log"]:
        video_save_dir = save_dir
        camera_config = get_camera_config(args["camera"]["head_camera_type"])
        video_size = str(camera_config["w"]) + "x" + str(camera_config["h"])
        video_save_dir.mkdir(parents=True, exist_ok=True)
        args["eval_video_save_dir"] = video_save_dir

    # output camera config
    clean_print("============= Config =============\n")
    clean_print("\033[95mMessy Table:\033[0m " + str(args["domain_randomization"]["cluttered_table"]))
    clean_print("\033[95mRandom Background:\033[0m " + str(args["domain_randomization"]["random_background"]))
    if args["domain_randomization"]["random_background"]:
        clean_print(" - Clean Background Rate: " + str(args["domain_randomization"]["clean_background_rate"]))
    clean_print("\033[95mRandom Light:\033[0m " + str(args["domain_randomization"]["random_light"]))
    if args["domain_randomization"]["random_light"]:
        clean_print(" - Crazy Random Light Rate: " + str(args["domain_randomization"]["crazy_random_light_rate"]))
    clean_print("\033[95mRandom Table Height:\033[0m " + str(args["domain_randomization"]["random_table_height"]))
    clean_print("\033[95mRandom Head Camera Distance:\033[0m " + str(args["domain_randomization"]["random_head_camera_dis"]))

    clean_print("\033[94mHead Camera Config:\033[0m " + str(args["camera"]["head_camera_type"]) + f", " +
          str(args["camera"]["collect_head_camera"]))
    clean_print("\033[94mWrist Camera Config:\033[0m " + str(args["camera"]["wrist_camera_type"]) + f", " +
          str(args["camera"]["collect_wrist_camera"]))
    clean_print("\033[94mEmbodiment Config:\033[0m " + embodiment_name)
    clean_print("\n==================================")

    TASK_ENV = class_decorator(args["task_name"])
    args["policy_name"] = policy_name
    usr_args["left_arm_dim"] = len(args["left_embodiment_config"]["arm_joints_name"][0])
    usr_args["right_arm_dim"] = len(args["right_embodiment_config"]["arm_joints_name"][1])

    seed = usr_args["seed"]

    st_seed = 100000 * (1 + seed)
    print(f"[DEBUG]: Start evaluation with seed: {st_seed}")
    suc_nums = []
    test_num = 10
    topk = 1

    model = get_model(usr_args)
    st_seed, suc_num = eval_policy(task_name,
                                   TASK_ENV,
                                   args,
                                   model,
                                   st_seed,
                                   test_num=test_num,
                                   video_size=video_size,
                                   instruction_type=instruction_type)
    suc_nums.append(suc_num)

    topk_success_rate = sorted(suc_nums, reverse=True)[:topk]

    file_path = os.path.join(save_dir, f"_result.txt")
    with open(file_path, "w") as file:
        file.write(f"Timestamp: {current_time}\n\n")
        file.write(f"Instruction Type: {instruction_type}\n\n")
        # file.write(str(task_reward) + '\n')
        file.write("\n".join(map(str, np.array(suc_nums) / test_num)))

    clean_print(f"Data has been saved to {file_path}")
    # return task_reward


def eval_policy(task_name,
                TASK_ENV,
                args,
                model,
                st_seed,
                test_num=100,
                video_size=None,
                instruction_type=None):
    clean_print(f"\033[34mTask Name: {args['task_name']}\033[0m")
    clean_print(f"\033[34mPolicy Name: {args['policy_name']}\033[0m")

    expert_check = True
    TASK_ENV.suc = 0
    TASK_ENV.test_num = 0

    now_id = 0
    succ_seed = 0
    suc_test_seed_list = []

    POLICY_NAME = args["policy_name"]
    eval_func = eval_function_decorator(POLICY_NAME, "eval")
    reset_func = eval_function_decorator(POLICY_NAME, "reset_model")

    now_seed = st_seed
    task_total_reward = 0
    clear_cache_freq = args["clear_cache_freq"]

    args["eval_mode"] = True
    
    # GT Data Dir logic
    gt_data_dir = args.get("gt_data_dir", "")
    gt_seeds = []
    if gt_data_dir:
        try:
            seed_file = os.path.join(os.path.dirname(gt_data_dir), "seed.txt")
            if os.path.exists(seed_file):
                with open(seed_file, "r") as f:
                    content = f.read().strip()
                    if content:
                        gt_seeds = [int(x) for x in content.split()]
                        clean_print(f"Loaded {len(gt_seeds)} seeds from {seed_file}")
                        # Update test_num to match available seeds/trajectories
                        test_num = len(gt_seeds)
            else:
                clean_print(f"Warning: Seed file not found at {seed_file}")
        except Exception as e:
            clean_print(f"Error loading seeds: {e}")

    # Start Eval Loop
    # We use 'episode_idx' to track the 0-indexed count of successful episodes we want to run
    episode_idx = 0 
    
    while episode_idx < test_num:
        render_freq = args["render_freq"]
        args["render_freq"] = 0

        # Determine current seed
        if gt_data_dir and gt_seeds:
            # Use specific seed from file
            if episode_idx < len(gt_seeds):
                now_seed = gt_seeds[episode_idx]
            else:
                break # Should not happen if test_num is set correctly
        else:
            # Default behavior (passed from st_seed) - already handled by incrementing now_seed in loops
            pass

        if expert_check:
            try:
                TASK_ENV.setup_demo(now_ep_num=now_id, seed=now_seed, is_test=True, **args)
                episode_info = TASK_ENV.play_once()
                TASK_ENV.close_env()
            except UnStableError as e:
                # clean_print(" -------------")
                # clean_print("Error: ", e)
                # clean_print(" -------------")
                TASK_ENV.close_env()
                if not (gt_data_dir and gt_seeds):
                     now_seed += 1
                args["render_freq"] = render_freq
                # If using fixed seeds, failing a seed is problematic. 
                # But usually we just retry or skip. If skip, we increment episode_idx.
                # Here original logic tries next seed for SAME episode ID.
                # For GT Replay, if setup fails, we probably can't replay that specific trajectory easily unless we fix the env.
                # Let's assume GT replay seeds are valid. If fail, maybe just skip this episode0/seed0 pair?
                # But mapping is strict: episode{i} <-> seed{i}.
                # If setup fails with seed{i}, we can't use episode{i}.
                # So we must skip both.
                if gt_data_dir and gt_seeds:
                     clean_print(f"\033[91mSetup failed for GT seed {now_seed}, skipping episode {episode_idx}\033[0m")
                     episode_idx += 1 # Skip this GT episode
                continue
            except Exception as e:
                # stack_trace = traceback.format_exc()
                # clean_print(" -------------")
                # clean_print("Error: ", e)
                # clean_print(" -------------")
                TASK_ENV.close_env() 
                if not (gt_data_dir and gt_seeds):
                     now_seed += 1
                args["render_freq"] = render_freq
                clean_print("error occurs !")
                if gt_data_dir and gt_seeds:
                     clean_print(f"\033[91mError for GT seed {now_seed}, skipping episode {episode_idx}\033[0m")
                     episode_idx += 1
                continue

        # If we are here, setup_demo succeeded (or expert_check passed)
        # Note: Original logic `if (not expert_check) or ...` 
        # Since we just ran setup_demo successfully above (if expert_check=True), we can proceed.
        
        # Original logic had a check `TASK_ENV.plan_success`.
        # For GT Replay, play_once might not be fully indicative if we just want to reset to initial state.
        # But let's keep original flow.
        
        if (not expert_check) or (TASK_ENV.plan_success and TASK_ENV.check_success()):
            # Valid seed found. 
            pass # We proceed to run eval
        else:
            # Plan failed (unsolvable with expert?)
            if not (gt_data_dir and gt_seeds):
                now_seed += 1
            args["render_freq"] = render_freq
            if gt_data_dir and gt_seeds:
                 clean_print(f"\033[91mExpert plan failed for GT seed {now_seed}, skipping episode {episode_idx}\033[0m")
                 episode_idx += 1
            continue

        args["render_freq"] = render_freq

        # Re-setup for actual eval
        TASK_ENV.setup_demo(now_ep_num=now_id, seed=now_seed, is_test=True, **args)
        episode_info_list = [episode_info["info"]]
        results = generate_episode_descriptions(args["task_name"], episode_info_list, test_num)
        instruction = np.random.choice(results[0][instruction_type])
        TASK_ENV.set_instruction(instruction=instruction)  # set language instruction

        if TASK_ENV.eval_video_path is not None:
             # FFMPEG setup (omitted for brevity, assume exists)
             pass 
        
        # FFMPEG Setup Block (simplified reconstruction from context)
        if TASK_ENV.eval_video_path is not None:
            ffmpeg = subprocess.Popen(
                [
                    "ffmpeg",
                    "-y",
                    "-loglevel",
                    "error",
                    "-f",
                    "rawvideo",
                    "-pixel_format",
                    "rgb24",
                    "-video_size",
                    video_size,
                    "-framerate",
                    "10",
                    "-i",
                    "-",
                    "-pix_fmt",
                    "yuv420p",
                    "-vcodec",
                    "libx264",
                    "-crf",
                    "23",
                    f"{TASK_ENV.eval_video_path}/episode{TASK_ENV.test_num}.mp4",
                ],
                stdin=subprocess.PIPE,
            )
            TASK_ENV._set_eval_video_ffmpeg(ffmpeg)


        succ = False
        reset_func(model)
        
        # Inject GT Action Path for current episode if applicable
        if gt_data_dir and gt_seeds:
            current_gt_path = os.path.join(gt_data_dir, f"episode{episode_idx}.hdf5")
            if hasattr(model, "set_gt_action_path"):
                model.set_gt_action_path(current_gt_path)
                clean_print(f"Set GT path to {current_gt_path} for Seed {now_seed}")

        while TASK_ENV.take_action_cnt < TASK_ENV.step_lim:
            observation = TASK_ENV.get_obs()
            eval_func(TASK_ENV, model, observation)
            if TASK_ENV.eval_success:
                succ = True
                break
        
        # Cleanup
        if TASK_ENV.eval_video_path is not None:
            TASK_ENV._del_eval_video_ffmpeg()

        if succ:
            TASK_ENV.suc += 1
            clean_print("\033[92mSuccess!\033[0m")
        else:
            clean_print("\033[91mFail!\033[0m")

        now_id += 1
        TASK_ENV.close_env(clear_cache=((succ_seed + 1) % clear_cache_freq == 0)) # Using succ_seed for cache signal?

        if TASK_ENV.render_freq:
            TASK_ENV.viewer.close()

        TASK_ENV.test_num += 1
        
        # Statistics
        clean_print(
            f"\033[93m{task_name}\033[0m | \033[94m{args['policy_name']}\033[0m | \033[92m{args['task_config']}\033[0m | \033[91m{args['ckpt_setting']}\033[0m\n"
            f"Success rate: \033[96m{TASK_ENV.suc}/{TASK_ENV.test_num}\033[0m => \033[95m{round(TASK_ENV.suc/TASK_ENV.test_num*100, 1)}%\033[0m, current seed: \033[90m{now_seed}\033[0m\n"
        )
        # TASK_ENV._take_picture()
        
        if not (gt_data_dir and gt_seeds):
            now_seed += 1
            
        # Increment our main progress counter
        episode_idx += 1 
        succ_seed += 1 # This was controlling the loop `while succ_seed < test_num`

    return now_seed, TASK_ENV.suc


def parse_args_and_config(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    parser.add_argument("--overrides", nargs=argparse.REMAINDER)
    parser.add_argument("--gt_data_dir", type=str, default="")
    args = parser.parse_args(args=args)

    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # Parse overrides
    def parse_override_pairs(pairs):
        override_dict = {}
        if pairs is None: return override_dict
        for i in range(0, len(pairs), 2):
            key = pairs[i].lstrip("--")
            value = pairs[i + 1]
            try:
                value = eval(value)
            except:
                pass
            override_dict[key] = value
        return override_dict

    if args.overrides:
        overrides = parse_override_pairs(args.overrides)
        config.update(overrides)
    
    # Inject gt_data_dir to config if present
    if args.gt_data_dir:
        config["gt_data_dir"] = args.gt_data_dir
        
    print(config)
    return config


if __name__ == "__main__":
    from test_render import Sapien_TEST
    Sapien_TEST()

    usr_args = parse_args_and_config()

    main(usr_args)
